{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MovieReview_SentimentAnalysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bf-8rCHx7z0Z",
        "colab_type": "text"
      },
      "source": [
        "# Movie Reviews Sentiment Analysis\n",
        "This project aims to predict the sentiment of the movie reviews using deep-learning, Convolutional Neural Net (CNN). We train the model with embeding layer and then we load pre-trained embeding layer to the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TEgzaJbFPGs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# unzip the dataset\n",
        "!tar -xvf 'review_polarity.tar.gz'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAKKfcip9q-d",
        "colab_type": "text"
      },
      "source": [
        "#Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLihSYwjIiyf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from os import listdir\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from collections import Counter\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense, Flatten, Embedding, Conv1D, MaxPool1D\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn.model_selection import train_test_split "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YN-4bkpPK4xo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "b4aa435f-faef-4e4a-e192-62c3acd5b84d"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MclJpR4jMIGA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stopingwords = set(stopwords.words('english'))\n",
        "table = str.maketrans('','',string.punctuation)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agFdckQ178zP",
        "colab_type": "text"
      },
      "source": [
        "#Dataset & pre-processing\n",
        "You can download the dataset from [here](https://raw.githubusercontent.com/jbrownlee/Datasets/master/review_polarity.tar.gz). \n",
        "\n",
        "After unzip the dataset, we can notice that it has two folders, one for postive reviews and one for negtaive reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z49CNJiQFPpV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# func to load the dataset\n",
        "def load_data(path):\n",
        "  file = open(path)\n",
        "  data = file.read()\n",
        "  file.close()\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFBpRx3AIZnQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# clean the data from stopping words and special characters\n",
        "def clean_data(data):\n",
        "  tokens = data.split()\n",
        "  tokens = [word for word in tokens if word not in stopingwords]\n",
        "  tokens = [word.translate(table) for word in tokens]\n",
        "  tokens = [word for word in tokens if len(word)>1 and  word.isalpha()]\n",
        "  return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hn_2Kp1UcsD9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# to calculate the number of words\n",
        "def add_to_vocab(data):\n",
        "  counter.update(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnqwJwqrFPvX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this function wrap up loading the data, clean it, calcualte the words numbers\n",
        "def process_data(path):\n",
        "  files_names = listdir(path)\n",
        "  for file_name in files_names:\n",
        "    file_path = path + '/' + file_name  \n",
        "    data = load_data(file_path)\n",
        "    tokens = clean_data(data)\n",
        "    add_to_vocab(tokens)\n",
        "    # print(file_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcVTYkZXFPym",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2c1db027-835e-4cd7-9b3b-5773b0c905ba"
      },
      "source": [
        "# find out how much words do we have in this dataset\n",
        "counter = Counter()\n",
        "\n",
        "neg_path = 'txt_sentoken/neg'\n",
        "pos_path = 'txt_sentoken/pos'\n",
        "process_data(neg_path)\n",
        "process_data(pos_path)\n",
        "# uncomment to see the vocabulries with its counts\n",
        "# print(counter)\n",
        "print(len(counter))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "46624\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NjEFyk_eQen",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "91eb14f2-d34a-4c8b-9ab8-19132de70a73"
      },
      "source": [
        "# filter the vocabulary based on the occurnes\n",
        "# we take the words with occurnes > 5\n",
        "print(len(counter))\n",
        "min_occur = 5\n",
        "vocabs = [k for k,c in counter.items() if c >= min_occur]\n",
        "print(len(vocabs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "46624\n",
            "14807\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXmmIKEzgw6z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# to save the new filtered vocabularies\n",
        "def save_list(lines, fileName):\n",
        "  data = '\\n'.join(lines)\n",
        "  file = open(fileName, 'w')\n",
        "  file.write(data)\n",
        "  file.close()\n",
        "\n",
        "save_list(vocabs, 'Vocabs.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uw7LUetiiP5p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "787364c4-d1a2-4304-a15f-6b08a6a7f089"
      },
      "source": [
        "# load the saved vocabularies\n",
        "file = open('Vocabs.txt')\n",
        "vocabs = file.read()\n",
        "file.close()\n",
        "print(len(vocabs.split('\\n')))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14807\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXiY1zy3Xgsp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c8cc5fd5-a28b-44a7-bb76-0c170341731e"
      },
      "source": [
        "# split the loaded vocabularies on new line \n",
        "loaded_vocabs = vocabs.split('\\n')\n",
        "len(loaded_vocabs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14807"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1JKSJFp6HXg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# func to filter one review based on the new vocabularies\n",
        "def filter_review(tokens):\n",
        "  filtered_tokens = [w for w in tokens if w in loaded_vocabs]\n",
        "  return ' '.join(filtered_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DX0BiVcziin1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# func for filter all the reviews\n",
        "def filter_reviews(path):\n",
        "  filtered_reviews = []\n",
        "  files_names = listdir(path)\n",
        "  for file_name in files_names:\n",
        "    file_path = path + '/' + file_name  \n",
        "    data = load_data(file_path)\n",
        "    tokens = clean_data(data)\n",
        "    filtered_review = filter_review(tokens)\n",
        "    filtered_reviews.append(filtered_review)\n",
        "  return filtered_reviews"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZM2zY4xqiiqn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "74e42555-dd4a-4d05-fced-da640ee4a2c7"
      },
      "source": [
        "# filter the negative and the postive reviews based on the occurnes of the vocabularies that we saved\n",
        "neg_path = 'txt_sentoken/neg'\n",
        "pos_path = 'txt_sentoken/pos'\n",
        "neg_reviews = filter_reviews(neg_path)\n",
        "print(len(neg_reviews))\n",
        "print(neg_reviews[:2])\n",
        "save_list(neg_reviews, 'neg_reviews.txt')\n",
        "# postives\n",
        "pos_reviews = filter_reviews(pos_path)\n",
        "print(len(pos_reviews))\n",
        "print(pos_reviews[:2])\n",
        "save_list(pos_reviews, 'pos_reviews.txt')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n",
            "['whether would considered probably depends would ask fan original series recently retired followup well even numbered entries film series however never one folks store away trek get relative merits vs data somewhere along line star trek film series began seem directed latter category star trek generations may natural conclusion direction production values may high writing frequently appalling instead script collection references injokes characters generations opens late century members original enterprise crew including captain james kirk william shatner present latest ship bear name sooner maiden voyage distress signal brings mysterious energy among rescued alien dr malcolm mcdowell back real world years later still trying get back encounters enterprise one led captain jeanluc picard patrick stewart plan involves destroying star inhabited planet hope saving million people historic meeting two enterprise think insult fans star trek suggest certain extent particulars plot really important elements trek film chance visit old friends involved truly interesting story considered even interpretation assumes characterization consistent generations happen data brent spiner particularly sloppy writing films major subplot data decides use chip give human emotions chip possession year motivation taking drastic perhaps dangerous step get joke chip place spiner gets lot fun new emotions point hes longer data know rest next generation cast fares better theyre screen little theyre simply window dressing story really new crew kirk picard cowboy politician sharing screen huge surprise william shatner blows patrick stewart away perhaps knows last part made legend shatner looks like hes time life stewart hand borders saddled lackluster motivations everyone else cast may bad sign hopes turn new cast kind franchise original cast clearly ready big screen players contrast prologue features scotty james walter highlight film characters become part popular mythology take enterprise moment distress moment high energy thirty years history create moment like thats time new crew certainly help plot stuck convoluted badly constructed need leave trail bread find way also loaded little gags aimed familiar enough series turn friends shared nudge recognition wonder whether anyone coming cold trek universe point would anything baffled bored going waste malcolm mcdowell could great villain given much compelling reason obsession plenty details scientific continuity pick would ignore generations major flaws however would like make one recommendation enterprise crew although think century space travel involve rolling around floor battle think audience might need journey one bumpy ride', 'feel sorry financial waterworld supposedly expensive film ever made million also one stupidest boy get ripped fact film really movie amusement park ride plot even another case cliches strung together well something thats painful watch relishes flaws kevin costner stars man kind mutant half fish alternate earth completely covered water polar ice caps hes referred would appropriate title hes cold rigid strict hes supposed good guy small floating village sorts dirt money introduces us important characters first meet one villains soontobe heroine child prodigy wacky inventor group primitive people somehow technology today also tools used pirates vikings immediately films biggest flaw apparent people primitive highly advanced seem like anyone read yet airplanes things powered airplanes keep flying reached dry land also planet completely covered water get materials make things im sorry im checking brain door going movies elements like cannot go unnoticed draw attention away story confuse us potential good pirate story set medieval times modern technology mix two together make sense really much plot story moves quickly never takes time explain anything thing learn prodigy child enola tattoo back supposedly map put tattoo come taken long figure never get answer stupid action movie whoever get girl victor sorts reach make long boring stupid plain bad story short escapes woman named helen tripplehorn enola sail long time strange people process fight lead idiot villain deacon hopper kidnaps enola story becomes overly grand adventure taking army goons rescuing enola bringing salvation original even special effects action adventure film boring single character likable therefore neither plot dennis hopper completely rips jack nicholsons joker costner plain rips still waterworld professionally made good production design original idea substance hope make sequel']\n",
            "1000\n",
            "['melvin udall heartless man spends days inside manhattan apartment writing romance novels also seems melvin never change one day favorite restaurant little mean normal waitress serve carol played perfection lovely sexy helen hunt threatens serve shut son shut make matters considerably worse melvin obsessive compulsive disorder one day gay artist neighbor simon greg talk soup fame oscarworthy role dog threatens dismiss door dog meets garbage soon simon sadly beaten ray cuba gooding jr simons agent takes dog melvin melvin dog rather car trip involves simon carol melvin learns emerge shell jack nicholson gives yet another oscarcaliber performance films cynical lead back work playing goofy president comic book instead playing jack playing full force adds years funniest comedy creative witty scathing film james brooks brooks gets performances entire cast winner every aspect truly delicious slice life', 'fellow americans movie first glance looks little substance movie weve seen million times two lifetime rivals thrown together fun begins exactly happened movie fortunately managed interesting funny way movie starts quick mean quick glance two presidents russell kramer jack lemmon matt douglas james garner william dan aykroyd ted john heard new president vice president scandal arises involving positive buried years ago finds kramer everyone wants kramer douglas dead movie exceptional many reasons one found people lemmon garner good together worked well unit mirrored perfectly one ladies man one old man ill let also found people know parts officials well seemed garner played almost exactly role played gentleman except experience counts lot']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuCCv_55EUUE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4e502451-19c1-4b82-8897-70451edc9599"
      },
      "source": [
        "# combine the negative and postive review togather as train data\n",
        "train_data = neg_reviews + pos_reviews\n",
        "print(len(train_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZwvCp9gMt9q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "42f6c129-ea06-4aa1-befe-ce3c0e5a1bcc"
      },
      "source": [
        "# define the labels for the reviews\n",
        "# 0: negtaive review\n",
        "# 1: postive review\n",
        "neg_labels = np.zeros((1,len(neg_reviews)))\n",
        "pos_labels = np.ones((1,len(pos_reviews)))\n",
        "labels = np.concatenate((neg_labels,pos_labels),axis=1).reshape(-1,1)\n",
        "labels.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmtB5LwZ8F4u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fit the tokenizer on the train data \n",
        "tokeniser = Tokenizer()\n",
        "tokeniser.fit_on_texts(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqXGjdTETAby",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# uncomment this to see each word with its counts\n",
        "# tokeniser.word_counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_S6nBcBuSiAB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "53169080-6e48-4acb-fc18-802cd99a3508"
      },
      "source": [
        "# make sure the output of the tokenzier has the same saved vocab counts \n",
        "len(tokeniser.word_counts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14807"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IN6QZCHBiQBf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert the words into numbers\n",
        "encoded_docs = tokeniser.texts_to_sequences(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fL-5XuUiHLwn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# padd the reviews to the maxium length we have in the dataset\n",
        "max_length = np.array([len(review) for review in encoded_docs]).max()\n",
        "padded_reviews = pad_sequences(encoded_docs,maxlen=max_length,padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLV7ql2QIW7o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1de2885b-2f84-4fca-920b-5456b46ab023"
      },
      "source": [
        "len(padded_reviews[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1238"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLcXI6H-On-M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "29c5fa8f-b3ef-4f02-dba1-d9e4c56849dd"
      },
      "source": [
        "padded_reviews.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 1238)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBd3zr6ghCfq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split the data into train + test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(padded_reviews, labels, test_size=0.3,random_state=7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5yFDcarIXA2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The vocabulary size is the total number of words in our vocabulary, plus one for unknown words.\n",
        "vocabs_num = len(vocabs.split('\\n')) + 1\n",
        "vocabs_dim = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80P68LWlJgx8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "2e3b8532-681c-4ae8-c765-35a108b53271"
      },
      "source": [
        "# this is the model we use for Sentiment Analysis\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocabs_num, vocabs_dim, input_length=max_length))\n",
        "model.add(Conv1D(64,8,activation='relu'))\n",
        "model.add(MaxPool1D(2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 1238, 100)         1480800   \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 1231, 64)          51264     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 615, 64)           0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 39360)             0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                393610    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 1,925,685\n",
            "Trainable params: 1,925,685\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJCG68_CJg0k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        },
        "outputId": "6068d81e-a845-43bf-db22-5d45b155c013"
      },
      "source": [
        "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'])\n",
        "model.fit(x_train,y_train,batch_size=32,epochs=20,verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "44/44 [==============================] - 14s 326ms/step - loss: 0.6918 - acc: 0.5214\n",
            "Epoch 2/20\n",
            "44/44 [==============================] - 14s 325ms/step - loss: 0.6266 - acc: 0.6586\n",
            "Epoch 3/20\n",
            "44/44 [==============================] - 14s 327ms/step - loss: 0.2147 - acc: 0.9529\n",
            "Epoch 4/20\n",
            "44/44 [==============================] - 14s 328ms/step - loss: 0.0162 - acc: 1.0000\n",
            "Epoch 5/20\n",
            "44/44 [==============================] - 14s 329ms/step - loss: 0.0041 - acc: 1.0000\n",
            "Epoch 6/20\n",
            "44/44 [==============================] - 14s 326ms/step - loss: 0.0022 - acc: 1.0000\n",
            "Epoch 7/20\n",
            "44/44 [==============================] - 14s 326ms/step - loss: 0.0015 - acc: 1.0000\n",
            "Epoch 8/20\n",
            "44/44 [==============================] - 14s 326ms/step - loss: 0.0011 - acc: 1.0000\n",
            "Epoch 9/20\n",
            "44/44 [==============================] - 14s 326ms/step - loss: 8.7109e-04 - acc: 1.0000\n",
            "Epoch 10/20\n",
            "44/44 [==============================] - 14s 326ms/step - loss: 7.0342e-04 - acc: 1.0000\n",
            "Epoch 11/20\n",
            "44/44 [==============================] - 14s 327ms/step - loss: 5.7668e-04 - acc: 1.0000\n",
            "Epoch 12/20\n",
            "44/44 [==============================] - 14s 326ms/step - loss: 4.7714e-04 - acc: 1.0000\n",
            "Epoch 13/20\n",
            "44/44 [==============================] - 14s 326ms/step - loss: 3.9124e-04 - acc: 1.0000\n",
            "Epoch 14/20\n",
            "44/44 [==============================] - 14s 324ms/step - loss: 3.2536e-04 - acc: 1.0000\n",
            "Epoch 15/20\n",
            "44/44 [==============================] - 14s 325ms/step - loss: 2.6916e-04 - acc: 1.0000\n",
            "Epoch 16/20\n",
            "44/44 [==============================] - 14s 325ms/step - loss: 2.2472e-04 - acc: 1.0000\n",
            "Epoch 17/20\n",
            "44/44 [==============================] - 14s 326ms/step - loss: 1.8944e-04 - acc: 1.0000\n",
            "Epoch 18/20\n",
            "44/44 [==============================] - 14s 325ms/step - loss: 1.6113e-04 - acc: 1.0000\n",
            "Epoch 19/20\n",
            "44/44 [==============================] - 14s 326ms/step - loss: 1.3800e-04 - acc: 1.0000\n",
            "Epoch 20/20\n",
            "44/44 [==============================] - 14s 326ms/step - loss: 1.1964e-04 - acc: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f79b51285f8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-5eL2RKJg4A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e8ad3d8d-2b69-4d8b-9af6-c63a95109483"
      },
      "source": [
        "# evalute the trained model, we get 86% accuracy (Great!)\n",
        "loss, acc = model.evaluate(x_test, y_test)\n",
        "print('Test Accuracy: %f' % (acc*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19/19 [==============================] - 1s 77ms/step - loss: 0.5126 - acc: 0.8600\n",
            "Test Accuracy: 86.000001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWnWm8OHBr6o",
        "colab_type": "text"
      },
      "source": [
        "# train stand_alone word2vec\n",
        "in this part, we will train a stand-alone embeding layer then load it to the model.\n",
        "\n",
        "The word2vec algorithm processes documents sentence by sentence. \n",
        "\n",
        "This means we will preserve the sentence-based structure during cleaning.\n",
        "\n",
        " this means we need tokens per sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAItXjRdU8-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models import Word2Vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Biyk3lydY-rE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# func to process the data\n",
        "def data_word2vec(path):\n",
        "  reviews_tokens = []\n",
        "  files_names = listdir(path)\n",
        "  for file_name in files_names:\n",
        "    file_path = path + '/' + file_name  \n",
        "    data = load_data(file_path)\n",
        "    tokens = clean_data(data)\n",
        "    reviews_tokens.append(tokens)\n",
        "  return reviews_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVJBAX_cZNCO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3aebc24b-1187-4df1-d419-93d2684ac7ed"
      },
      "source": [
        "# process the data\n",
        "neg_path = 'txt_sentoken/neg'\n",
        "pos_path = 'txt_sentoken/pos'\n",
        "new_data_neg = data_word2vec(neg_path)\n",
        "new_data_pos = data_word2vec(pos_path)\n",
        "train_data = new_data_neg + new_data_pos\n",
        "print(len(train_data))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUR0gQsmVTnt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fbf7b567-de58-4e30-c564-2233cbda4346"
      },
      "source": [
        "# define the stand-alone embeding layer\n",
        "standAlone_model = Word2Vec(train_data,size=100,window=5,min_count=1,workers=8)\n",
        "print(standAlone_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word2Vec(vocab=46624, size=100, alpha=0.025)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwEO7pYgaVkk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# standAlone_model.wv.vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ouvkfyw9aYCY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "outputId": "1bec3615-5074-4330-cf8f-4085a56a75bc"
      },
      "source": [
        "# see the learned vector of the word \"example\"\n",
        "standAlone_model['example']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.4803404 , -0.7001595 ,  0.31508818, -0.03183432,  0.0448408 ,\n",
              "       -0.22982943, -0.01224964,  0.22862013,  0.502656  , -0.7864699 ,\n",
              "       -0.12819374, -0.47206038, -0.04850436,  0.24738428,  0.96677756,\n",
              "       -0.1405719 , -0.9670399 ,  0.11147805,  0.60952795,  1.1552802 ,\n",
              "        0.5080436 ,  0.6740551 ,  0.14654763, -0.6544406 ,  0.15083729,\n",
              "        0.2545559 , -0.20509699,  0.91583425,  0.10597903, -0.10086969,\n",
              "        0.1461346 , -0.17658238, -0.16476314,  0.3305362 ,  0.04102265,\n",
              "        0.35190314, -0.63464266,  0.5597246 ,  0.20803364, -0.37408945,\n",
              "       -0.27982354, -1.049409  , -0.3020725 ,  0.65368974, -0.25004718,\n",
              "        0.3431087 ,  0.55082375, -0.06871083, -0.30621013, -0.83140004,\n",
              "        0.6259254 , -0.872284  ,  0.33605662, -0.79621726, -0.6564979 ,\n",
              "       -0.51993096,  0.09395964,  0.95523053,  0.6880927 , -0.7408734 ,\n",
              "        0.900521  ,  0.00936002,  0.5925486 ,  0.05372783,  0.45151564,\n",
              "        0.3633122 ,  0.69971997,  0.32359302,  0.70402   , -1.041937  ,\n",
              "        0.10980549, -0.39312258,  0.39828536,  0.52035815,  2.0184557 ,\n",
              "        0.08239291,  0.56806   ,  0.12823452, -0.48777962,  0.65140563,\n",
              "       -0.5999603 ,  0.2969493 ,  0.00450242,  0.48979747, -0.51270926,\n",
              "        0.4093853 ,  0.2090678 ,  0.5525186 , -1.043535  ,  0.36164826,\n",
              "       -0.166442  , -0.15268458, -0.9028956 , -0.29905877,  0.50926507,\n",
              "       -0.27817187, -0.03159143,  0.08040135,  0.57827675,  1.1459708 ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYyYk5nZVTyw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "fb11a9c9-3fd2-48cb-d6fd-d33ba60941f3"
      },
      "source": [
        "# convert all the vocabulries we have into vectors using the stand alone embeding layer\n",
        "all_vectors = standAlone_model[standAlone_model.wv.vocab]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU_-ZXrwWEh4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "e6970818-648f-4644-a706-c21011308ef2"
      },
      "source": [
        "# save the weights of the embeding layer\n",
        "standAlone_model.wv.save_word2vec_format('word2vec.txt',binary=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQ6lqmz9DELT",
        "colab_type": "text"
      },
      "source": [
        "# Use Pre-trained Embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_bxjU5CWEkk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load embedding as a dict\n",
        "def load_embedding(filename):\n",
        "\t# load embedding into memory, skip first line\n",
        "\tfile = open(filename,'r')\n",
        "\tlines = file.readlines()[1:]\n",
        "\tfile.close()\n",
        "\t# create a map of words to vectors\n",
        "\tembedding = dict()\n",
        "\tfor line in lines:\n",
        "\t\tparts = line.split()\n",
        "\t\t# key is string word, value is numpy array for vector\n",
        "\t\tembedding[parts[0]] = np.asarray(parts[1:], dtype='float32')\n",
        "\treturn embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DseMnSP_WEoy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the saved weights of stand-alone embeding layer\n",
        "raw_embed = load_embedding('word2vec.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pY7WR9IXeQnx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a weight matrix for the Embedding layer from a loaded embedding\n",
        "def get_weight_matrix(embedding, vocab):\n",
        "\t# total vocabulary size plus 0 for unknown words\n",
        "\tvocab_size = len(vocab) + 1\n",
        "\t# define weight matrix dimensions with all 0\n",
        "\tweight_matrix = np.zeros((vocab_size, 100))\n",
        "\t# step vocab, store vectors using the Tokenizer's integer mapping\n",
        "\tfor word, i in vocab.items():\n",
        "\t\tweight_matrix[i] = embedding.get(word)\n",
        "\treturn weight_matrix\n",
        "\n",
        "embedding_vectors = get_weight_matrix(raw_embed, tokeniser.word_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-oUsSWVlMJ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the embeding layer after load the weights\n",
        "embedding_layer = Embedding(vocabs_num, vocabs_dim, weights=[embedding_vectors], input_length=max_length, trainable=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qE0hLV6VlMHB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "ce2d364a-a80b-4bd5-9bca-ae3b7c352be2"
      },
      "source": [
        "# the model we use with the stand-alone embeding layer\n",
        "model = Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(Conv1D(64,8,activation='relu'))\n",
        "model.add(MaxPool1D(2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 1238, 100)         1480800   \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 1231, 64)          51264     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1 (None, 615, 64)           0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 39360)             0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 10)                393610    \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 1,925,685\n",
            "Trainable params: 444,885\n",
            "Non-trainable params: 1,480,800\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7x_u0r4lME1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        },
        "outputId": "6de3d721-d41b-4fbe-8f9f-f8fd29023fa5"
      },
      "source": [
        "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'])\n",
        "model.fit(x_train,y_train,batch_size=32,epochs=20,verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "44/44 [==============================] - 10s 222ms/step - loss: 0.7158 - acc: 0.4650\n",
            "Epoch 2/20\n",
            "44/44 [==============================] - 10s 222ms/step - loss: 0.6932 - acc: 0.4943\n",
            "Epoch 3/20\n",
            "44/44 [==============================] - 10s 222ms/step - loss: 0.6931 - acc: 0.4936\n",
            "Epoch 4/20\n",
            "44/44 [==============================] - 10s 222ms/step - loss: 0.6937 - acc: 0.5300\n",
            "Epoch 5/20\n",
            "44/44 [==============================] - 10s 222ms/step - loss: 0.6934 - acc: 0.5264\n",
            "Epoch 6/20\n",
            "44/44 [==============================] - 10s 222ms/step - loss: 0.6931 - acc: 0.5214\n",
            "Epoch 7/20\n",
            "44/44 [==============================] - 10s 222ms/step - loss: 0.6924 - acc: 0.5314\n",
            "Epoch 8/20\n",
            "44/44 [==============================] - 10s 223ms/step - loss: 0.6889 - acc: 0.5271\n",
            "Epoch 9/20\n",
            "44/44 [==============================] - 10s 223ms/step - loss: 0.6779 - acc: 0.5886\n",
            "Epoch 10/20\n",
            "44/44 [==============================] - 10s 223ms/step - loss: 0.6818 - acc: 0.5579\n",
            "Epoch 11/20\n",
            "44/44 [==============================] - 10s 221ms/step - loss: 0.6668 - acc: 0.6079\n",
            "Epoch 12/20\n",
            "44/44 [==============================] - 10s 222ms/step - loss: 0.6624 - acc: 0.6293\n",
            "Epoch 13/20\n",
            "44/44 [==============================] - 10s 222ms/step - loss: 0.6329 - acc: 0.6571\n",
            "Epoch 14/20\n",
            "44/44 [==============================] - 10s 221ms/step - loss: 0.6049 - acc: 0.7021\n",
            "Epoch 15/20\n",
            "44/44 [==============================] - 10s 222ms/step - loss: 0.5738 - acc: 0.7364\n",
            "Epoch 16/20\n",
            "44/44 [==============================] - 10s 222ms/step - loss: 0.5188 - acc: 0.7743\n",
            "Epoch 17/20\n",
            "44/44 [==============================] - 10s 222ms/step - loss: 0.4750 - acc: 0.8186\n",
            "Epoch 18/20\n",
            "44/44 [==============================] - 10s 222ms/step - loss: 0.4375 - acc: 0.8464\n",
            "Epoch 19/20\n",
            "44/44 [==============================] - 10s 221ms/step - loss: 0.3962 - acc: 0.8736\n",
            "Epoch 20/20\n",
            "44/44 [==============================] - 10s 222ms/step - loss: 0.3623 - acc: 0.9050\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f79ac3087b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBpw6VgjnO6M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7efc7321-672f-4e30-b150-bb7c96366d19"
      },
      "source": [
        "loss, acc = model.evaluate(x_test, y_test)\n",
        "print('Test Accuracy: %f' % (acc*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19/19 [==============================] - 1s 77ms/step - loss: 1.2240 - acc: 0.5567\n",
            "Test Accuracy: 55.666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPCRN-1jDjax",
        "colab_type": "text"
      },
      "source": [
        "#Reasults\n",
        "We get worse reaslts using stand-alone embeding layer than training the embeding layer with the whole model. It might need tunning hyper-parameters or maybe use better pre-trained model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGyHDK9A7Yiu",
        "colab_type": "text"
      },
      "source": [
        "#Reference\n",
        "https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLETW60enPCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}